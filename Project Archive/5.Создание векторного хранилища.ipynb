{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad351c65-70d2-46f2-a326-861f4d730ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#В этом ноутбуке находится код создания векторного хранилица для одного года."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0851201-edd7-405b-a631-bae6a63810a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T21:58:56.441479Z",
     "iopub.status.busy": "2024-06-12T21:58:56.439931Z",
     "iopub.status.idle": "2024-06-12T21:58:57.979914Z",
     "shell.execute_reply": "2024-06-12T21:58:57.979078Z",
     "shell.execute_reply.started": "2024-06-12T21:58:56.441436Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20843    «Синьхуа -агентство» сообщило, что, согласно о...\n",
       "20844    Проект поправки к трудовым законодательству ещ...\n",
       "20845    ／ Reporter Fang Weiduo / Taipei сообщает, что ...\n",
       "20846    В первый день начала школы американских школ С...\n",
       "20847    Credit Suisse заявила, что он согласился запла...\n",
       "                               ...                        \n",
       "37043    Стресс на рабочем месте, как говорят, является...\n",
       "37044    Россия чувствует, что демократическая команда ...\n",
       "37045    В 2006 году участники проекта НЛО Хантер изуча...\n",
       "37046    Студенты показали, что многие женщины сталкива...\n",
       "37047    Турецкие ВВС были успешно разрушены Дайсом (Ис...\n",
       "Name: translated_text, Length: 981, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "f = pd.read_csv('/home/jupyter/datasphere/project/rag_east_data.csv')\n",
    "filtered_df = f[f['year'] == 2016]  # или любой другой год из 2017, 2019, 2022, 2023\n",
    "\n",
    "docs = filtered_df['translated_text']\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf62ab-8973-473b-a065-e2b2bb156595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install yandex_chain \n",
    "%pip install -U langchain-community\n",
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ae7300-3c18-4a6b-801e-641b72e7bad9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T21:59:37.573226Z",
     "iopub.status.busy": "2024-06-12T21:59:37.571841Z",
     "iopub.status.idle": "2024-06-12T22:17:48.352012Z",
     "shell.execute_reply": "2024-06-12T22:17:48.351255Z",
     "shell.execute_reply.started": "2024-06-12T21:59:37.573182Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Обработка документов: 100%|██████████| 981/981 [00:00<00:00, 1532.41документ/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Создание векторного хранилища: 100%|██████████| 984/984 [18:06<00:00,  1.10s/документ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время на выполнение vectorstore_time: 18.107376035054525 минут\n",
      "Готово\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from yandex_chain import YandexLLM, YandexEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def split_document(doc, max_tokens=1200):\n",
    "    words = word_tokenize(doc)\n",
    "    return [' '.join(words[i:i + max_tokens]) for i in range(0, len(words), max_tokens)]\n",
    "\n",
    "# Настройка Yandex Embeddings\n",
    "folder_id = os.environ['folder_id']\n",
    "api_key = os.environ['my-API']\n",
    "embeddings = YandexEmbeddings(folder_id=folder_id, api_key=api_key)\n",
    "\n",
    "\n",
    "docs_with_progress = []\n",
    "for i, doc in enumerate(tqdm(docs, desc=\"Обработка документов\", unit=\"документ\")):\n",
    "    try:\n",
    "        if len(word_tokenize(doc)) > 1200:     # у эмбеддингов ограничение 2048 токенов (сабтокены), а у nltk по-другому\n",
    "            split_docs = split_document(doc)\n",
    "            docs_with_progress.extend(split_docs)\n",
    "        else:\n",
    "            docs_with_progress.append(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке документа {i}: {e}\")\n",
    "        continue\n",
    "print(len(docs_with_progress))\n",
    "\n",
    "start_time = time.time()\n",
    "vectorstore = None\n",
    "batch_size = 100  # Количество документов а батче\n",
    "current_batch = []\n",
    "batch_index = 0\n",
    "\n",
    "for i, doc in enumerate(tqdm(docs_with_progress, desc=\"Создание векторного хранилища\", unit=\"документ\")):\n",
    "    current_batch.append(doc)\n",
    "    if len(current_batch) >= batch_size or i == len(docs_with_progress) - 1:\n",
    "        try:\n",
    "            if vectorstore is None:\n",
    "                vectorstore = FAISS.from_texts(current_batch, embedding=embeddings)\n",
    "            else:\n",
    "                vectorstore.add_texts(current_batch)\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка на итерации {i} при добавлении документа в векторное хранилище: {e}\")\n",
    "            continue\n",
    "            \n",
    "            # Сохранение промежуточных результатов\n",
    "        faiss.write_index(vectorstore.index, f\"faiss_2016_index.index\")\n",
    "        with open(f\"faiss_2016_index.pkl\", \"wb\") as f:\n",
    "            pickle.dump(vectorstore, f)\n",
    "\n",
    "        current_batch = []  # Очистка текущей партии\n",
    "        batch_index += 1  # Увеличение индекса батча\n",
    "            \n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "vectorstore_time = end_time - start_time\n",
    "print(f\"Время на выполнение vectorstore_time: {vectorstore_time/60} минут\")\n",
    "\n",
    "print(\"Готово\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
